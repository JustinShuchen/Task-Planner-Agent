import os
import json
import random
from typing import List, Dict
import numpy as np
from PIL import Image
from tqdm import tqdm

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import models, transforms
from transformers import AutoTokenizer, AutoModel
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# ======== å›ºå®šè·¯å¾„ï¼ˆå·²æŒ‰ä½ çš„å®žé™…æƒ…å†µä¿®æ”¹ï¼‰========
json_dir = r"c:\Users\hp\Desktop\surf\final_dataset\json"  # åŒ…å«å¤šä¸ªJSONæ–‡ä»¶çš„ç›®å½•
img_dir = r"c:\Users\hp\Desktop\surf\final_dataset"     # åŒ…å«æ‰€æœ‰å›¾ç‰‡çš„ç›®å½•

# ======== å¯è°ƒå‚æ•° ========
BERT_MODEL       = "bert-base-uncased"   
IMG_BACKBONE     = "resnet18"            
MAX_LEN          = 64
IMG_SIZE         = 224
EPOCHS           = 10
BATCH_SIZE       = 16
LR               = 2e-4
SEED             = 4
FINETUNE_TEXT    = False                 
FINETUNE_IMAGE   = False                 

# ======== å·¥å…·å‡½æ•° ========
def set_seed(seed=SEED):
    random.seed(seed); np.random.seed(seed)
    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)

def load_records_from_dir(json_dir: str) -> List[Dict]:
    """ä»Žç›®å½•åŠ è½½æ‰€æœ‰JSONæ–‡ä»¶å¹¶åˆå¹¶è®°å½•"""
    records = []
    # èŽ·å–ç›®å½•ä¸­æ‰€æœ‰JSONæ–‡ä»¶
    for filename in os.listdir(json_dir):
        if filename.endswith('.json'):
            json_path = os.path.join(json_dir, filename)
            try:
                with open(json_path, "r", encoding="utf-8") as f:
                    file_records = json.load(f)
                    # ç¡®ä¿åŠ è½½çš„æ˜¯åˆ—è¡¨
                    if isinstance(file_records, list):
                        records.extend(file_records)
                        print(f"åŠ è½½æˆåŠŸ: {filename}, åŒ…å« {len(file_records)} æ¡è®°å½•")
                    else:
                        print(f"è­¦å‘Š: {filename} å†…å®¹ä¸æ˜¯åˆ—è¡¨ï¼Œå·²è·³è¿‡")
            except Exception as e:
                print(f"åŠ è½½ {filename} å¤±è´¥: {str(e)}")
    
    if not records:
        raise ValueError("æœªä»ŽJSONç›®å½•åŠ è½½åˆ°ä»»ä½•è®°å½•ï¼Œè¯·æ£€æŸ¥ç›®å½•å’Œæ–‡ä»¶æ ¼å¼")
    
    # è½»é‡æ ¡éªŒå­—æ®µå
    required_fields = ("prompt", "image_path", "task_type")
    for field in required_fields:
        if field not in records[0]:
            raise KeyError(f"JSONå­—æ®µç¼ºå¤±: éœ€è¦åŒ…å« '{field}'ï¼Œè¯·æ£€æŸ¥ä½ çš„æ•°æ®ã€‚")
    
    return records

def build_label_maps(records: List[Dict]):
    classes = sorted(list({r["task_type"] for r in records}))
    label2id = {c:i for i,c in enumerate(classes)}
    id2label = {i:c for c,i in label2id.items()}
    return label2id, id2label

def split_data(records, test_size=0.2, val_size=0.1, seed=SEED):
    y = [r["task_type"] for r in records]
    train_val, test = train_test_split(records, test_size=test_size, random_state=seed, stratify=y)
    y_tv = [r["task_type"] for r in train_val]
    val_ratio = val_size / (1 - test_size)
    train, val = train_test_split(train_val, test_size=val_ratio, random_state=seed, stratify=y_tv)
    return train, val, test
from pathlib import Path

ALT_EXTS = [".png", ".jpg", ".jpeg"]  # å…è®¸çš„å›¾ç‰‡æ‰©å±•å

def _normalize_rel(p: str) -> str:
    """ç»Ÿä¸€ image_path å†™æ³•ï¼šåæ–œæ â†’æ­£æ–œæ ã€åŽ»é¦–å°¾ç©ºæ ¼"""
    return (p or "").replace("\\", "/").strip()

def _exists_with_exts(abs_path: Path) -> bool:
    """å…è®¸ .png/.jpg/.jpeg å…œåº•"""
    if abs_path.exists():
        return True
    stem = abs_path.with_suffix("")
    for ext in ALT_EXTS:
        if (stem.with_suffix(ext)).exists():
            return True
    return False

def filter_records_by_image(records: List[Dict], images_root: str) -> List[Dict]:
    """åªä¿ç•™èƒ½åœ¨ç£ç›˜ä¸Šæ‰¾åˆ°å›¾ç‰‡çš„æ ·æœ¬ï¼›é¡ºå¸¦è§„èŒƒåŒ– image_path"""
    root = Path(images_root)
    keep = []
    drop = 0
    for r in records:
        rel = _normalize_rel(r.get("image_path", ""))
        if not rel:
            drop += 1
            continue
        abs_path = (root / rel)
        if _exists_with_exts(abs_path):
            r["image_path"] = rel  # è§„èŒƒåŒ–å†™å›ž
            keep.append(r)
        else:
            drop += 1
    print(f"ðŸ” è®°å½•ç­›é€‰ï¼šåŽŸå§‹ {len(records)} æ¡ â†’ ä¿ç•™ {len(keep)} æ¡ï¼Œå¿½ç•¥æ— å›¾ {drop} æ¡")
    if not keep:
        raise ValueError("ç­›é€‰åŽæ²¡æœ‰å¯ç”¨æ ·æœ¬ï¼šè¯·æ£€æŸ¥ img_dir / image_path æ˜¯å¦æ­£ç¡®ã€‚")
    return keep

# ======== æ•°æ®é›† ========
class MultiModalDataset(Dataset):
    def __init__(self, recs, label2id, tokenizer, images_root, max_len=64, img_size=224):
        self.recs = recs
        self.label2id = label2id
        self.tokenizer = tokenizer
        self.images_root = images_root
        self.max_len = max_len
        self.tf = transforms.Compose([
            transforms.Resize((img_size, img_size)),
            transforms.ToTensor(),
            transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
        ])

    def __len__(self):
        return len(self.recs)

    def __getitem__(self, i):
        r = self.recs[i]
        text = r["prompt"]
        # å…¼å®¹ç›¸å¯¹/ç»å¯¹è·¯å¾„
       # å…¼å®¹ç›¸å¯¹/ç»å¯¹è·¯å¾„
        img_path = r["image_path"]
        img_path = _normalize_rel(img_path)              # æ–°å¢žï¼šç»Ÿä¸€è·¯å¾„å†™æ³•
        if not os.path.isabs(img_path):
          img_path = os.path.join(self.images_root, img_path)
        if not os.path.exists(img_path):
    # å…œåº•å°è¯•å…¶ä»–æ‰©å±•å
          stem, ext = os.path.splitext(img_path)
          found = False
          for e in ALT_EXTS:
           cand = stem + e
           if os.path.exists(cand):
            img_path = cand
            found = True
            break
          if not found:
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°å›¾ç‰‡: {img_path}")

        # æ–‡æœ¬
        enc = self.tokenizer(text, padding="max_length", truncation=True,
                             max_length=self.max_len, return_tensors="pt")
        item = {k: v.squeeze(0) for k, v in enc.items()}

        # å›¾åƒ
        img = Image.open(img_path).convert("RGB")
        img = self.tf(img)
        item["pixel_values"] = img

        # æ ‡ç­¾
        item["label"] = torch.tensor(self.label2id[r["task_type"]], dtype=torch.long)
        return item

# ======== ç¼–ç å™¨ ========
class TextEncoder(nn.Module):
    def __init__(self, model_name=BERT_MODEL, finetune=FINETUNE_TEXT):
        super().__init__()
        self.bert = AutoModel.from_pretrained(model_name)
        if not finetune:
            for p in self.bert.parameters():
                p.requires_grad = False
        self.out_dim = self.bert.config.hidden_size

    def forward(self, input_ids, attention_mask):
        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        if hasattr(out, "pooler_output") and out.pooler_output is not None:
            return out.pooler_output
        return out.last_hidden_state[:, 0, :]  # é€€åŒ–åˆ°CLS

class ImageEncoder(nn.Module):
    def __init__(self, backbone=IMG_BACKBONE, finetune=FINETUNE_IMAGE):
        super().__init__()
        if backbone == "resnet50":
            net = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
            feat_dim = 2048
        else:
            net = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)
            feat_dim = 512
        self.out_dim = feat_dim
        self.cnn = nn.Sequential(*list(net.children())[:-1])  # åŽ»æŽ‰fc
        if not finetune:
            for p in self.cnn.parameters():
                p.requires_grad = False

    def forward(self, x):
        f = self.cnn(x)      # [B,C,1,1]
        return f.flatten(1)  # [B,C]

# ======== å…¨è¿žæŽ¥èžåˆåˆ†ç±»å™¨ ========
class FusionMLP(nn.Module):
    def __init__(self, text_dim, img_dim, num_classes):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(text_dim + img_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )

    def forward(self, t, i):
        return self.net(torch.cat([t, i], dim=1))

# ======== è¯„ä¼° ========
@torch.no_grad()
def evaluate(loader, txt_enc, img_enc, clf, device):
    txt_enc.eval(); img_enc.eval(); clf.eval()
    tot, cor = 0, 0
    for batch in loader:
        batch = {k:(v.to(device) if isinstance(v, torch.Tensor) else v) for k,v in batch.items()}
        t = txt_enc(batch["input_ids"], batch["attention_mask"])
        im = img_enc(batch["pixel_values"])
        logits = clf(t, im)
        pred = logits.argmax(1)
        cor += (pred == batch["label"]).sum().item()
        tot += batch["label"].size(0)
    return cor / max(tot, 1)

@torch.no_grad()
def test_and_report(loader, txt_enc, img_enc, clf, device, id2label):
    txt_enc.eval(); img_enc.eval(); clf.eval()
    y_true, y_pred = [], []
    for batch in loader:
        batch = {k:(v.to(device) if isinstance(v, torch.Tensor) else v) for k,v in batch.items()}
        t = txt_enc(batch["input_ids"], batch["attention_mask"])
        im = img_enc(batch["pixel_values"])
        logits = clf(t, im)
        pred = logits.argmax(1).cpu().numpy()
        y_pred.extend(pred.tolist())
        y_true.extend(batch["label"].cpu().numpy().tolist())
    acc = accuracy_score(y_true, y_pred)
    print(f"[TEST] accuracy={acc:.4f}")
    print(classification_report(y_true, y_pred, target_names=[id2label[i] for i in range(len(id2label))]))
    print("Confusion matrix:\n", confusion_matrix(y_true, y_pred))

# ======== ä¸»æµç¨‹ ========
def main():
    set_seed(SEED)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print("Device:", device)

    # 1) è¯»æ•°æ® + æ ‡ç­¾æ˜ å°„
    records = load_records_from_dir(json_dir)
    # >>> æ–°å¢žï¼šå¿½ç•¥åæ ·æœ¬ï¼ˆæ‰¾ä¸åˆ°å›¾ç‰‡çš„è®°å½•ï¼‰
    records = filter_records_by_image(records, img_dir)

    print(f"å…±åŠ è½½ {len(records)} æ¡è®°å½•ï¼ˆå·²è¿‡æ»¤æ— å›¾æ ·æœ¬ï¼‰")

    
    label2id, id2label = build_label_maps(records)
    with open("label_map.json", "w", encoding="utf-8") as f:
        json.dump({"label2id": label2id, "id2label": id2label}, f, ensure_ascii=False, indent=2)
    print(f"å…± {len(label2id)} ä¸ªç±»åˆ«ï¼š", list(label2id.keys()))

    # 2) åˆ’åˆ†æ•°æ®é›†
    train_rec, val_rec, test_rec = split_data(records, test_size=0.2, val_size=0.1, seed=SEED)
    print(f"Split -> train={len(train_rec)}, val={len(val_rec)}, test={len(test_rec)}")

    # 3) æž„å»ºæ•°æ®åŠ è½½å™¨
    tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL)
    train_ds = MultiModalDataset(train_rec, label2id, tokenizer, img_dir, MAX_LEN, IMG_SIZE)
    val_ds   = MultiModalDataset(val_rec,   label2id, tokenizer, img_dir, MAX_LEN, IMG_SIZE)
    test_ds  = MultiModalDataset(test_rec,  label2id, tokenizer, img_dir, MAX_LEN, IMG_SIZE)
    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)
    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=2)
    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2)

    # 4) æ¨¡åž‹
    txt_enc = TextEncoder(BERT_MODEL, FINETUNE_TEXT).to(device)
    img_enc = ImageEncoder(IMG_BACKBONE, FINETUNE_IMAGE).to(device)
    clf     = FusionMLP(txt_enc.out_dim, img_enc.out_dim, len(label2id)).to(device)

    params = list(filter(lambda p: p.requires_grad, list(txt_enc.parameters()) + list(img_enc.parameters()) + list(clf.parameters())))
    optimizer = torch.optim.AdamW(params, lr=LR)
    criterion = nn.CrossEntropyLoss()

    # 5) è®­ç»ƒ
    best_val, best_state = 0.0, None
    for ep in range(1, EPOCHS+1):
        txt_enc.train(); img_enc.train(); clf.train()
        total, correct, loss_sum = 0, 0, 0.0
        for batch in tqdm(train_loader, desc=f"Epoch {ep}/{EPOCHS}"):
            batch = {k:(v.to(device) if isinstance(v, torch.Tensor) else v) for k,v in batch.items()}
            optimizer.zero_grad()
            t = txt_enc(batch["input_ids"], batch["attention_mask"])
            im = img_enc(batch["pixel_values"])
            logits = clf(t, im)
            loss = criterion(logits, batch["label"])
            loss.backward()
            optimizer.step()

            loss_sum += loss.item() * batch["label"].size(0)
            pred = logits.argmax(1)
            correct += (pred == batch["label"]).sum().item()
            total += batch["label"].size(0)

        train_acc = correct / max(total,1)
        val_acc = evaluate(val_loader, txt_enc, img_enc, clf, device)
        print(f"[Epoch {ep}] train_acc={train_acc:.4f} val_acc={val_acc:.4f} loss={loss_sum/max(total,1):.4f}")

        if val_acc > best_val:
            best_val = val_acc
            best_state = {
                "txt": txt_enc.state_dict(),
                "img": img_enc.state_dict(),
                "clf": clf.state_dict()
            }
            torch.save(best_state, "best_mm_fc.pt")
            print("âœ“ Saved best checkpoint -> best_mm_fc.pt")

    # 6) æµ‹è¯•
    if best_state is not None:
        txt_enc.load_state_dict(best_state["txt"])
        img_enc.load_state_dict(best_state["img"])
        clf.load_state_dict(best_state["clf"])
    test_and_report(test_loader, txt_enc, img_enc, clf, device, id2label)

if __name__ == "__main__":
    

     
    main()
